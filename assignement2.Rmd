---
title: "assignement2"
author: "Lilach Herzog & Leslie Cohen"
date: "13 5 2022"
output: pdf_document
---

---
Classify the seed types using 2 classifying methods:

<<<<<<< HEAD

=======


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#preproccessing

##### Set up our environment and data:

```{r setup, results='hide'}
library(gmodels)
library(C50)
library(class) 
library(gmodels)

# read data
seeds <- read.csv("seeds.csv")

# Consider setting a seed for reproducible results

#to initialize a pseudo random number generator. 
set.seed(1234)
```

#### find the type

The aim: classify the seed types.
There are 3 types of seeds: 1,2,3.
we took a quick look at the data to try to find which parameter is important :

```{r}
str(seeds)
```

##### convert type from int variable to factor

we converted the class ("type") to a factor, as it is required by the C50 package.

```{r}
seeds$Type<-as.factor(seeds$Type)
```

```{r}
#the "table" function builds a contingency table of the counts at each combination of factor levels- the number of occurences of each lengh / width/perimeter etc and order them from the lower to the higher kernel/perimeter etc 

table(seeds$Kernel.Length) #majority of  1
table(seeds$Kernel.Width) #majority of  1
table(seeds$Kernel.Groove) #mix of  1 to 4
table(seeds$Perimeter) #majority of 1
table(seeds$Area) #majority of 1
table(seeds$Compactness) #mix of 1 and 2
table(seeds$Asymmetry.Coeff) #majority of 1

```
The only "repetitive variable" we found is kernel groove.


```{r}
# look at summary of the different kernel variables 
summary(seeds$Kernel.Groove)
summary(seeds$Kernel.Length)
summary(seeds$Kernel.Width)

```
the kernel groove varies between 4.5 and 6.55, the kernel lenght between 4.89 and 6.67
and the kernel width of seeds varies between 2.63 and 4.033. 

How can we associate that to the different types of seeds ??

```{r}
table(seeds$Type)
```



### split into training and test sets
Lets split into training and test sets:

```{r}

#  0.8% of the observations out of the total 199
train_sample <- sample(nrow(seeds), round(0.8*(nrow(seeds)),0))


str(train_sample) # the resulting train_sample object is a vector of 159 random integers


# split into train/test
# for the train we take the train sample
seeds_train <- seeds[train_sample, ]
#for the test, we take all the data except the train sample
seeds_test <- seeds[-train_sample, ]
```
There are 66 type 1, 68 type 2 and 65 type 3 from our table seeds, which means that we have about a third of each type in the data set.
For that we should check that we got about 1/3 of each type in each data set (training and test sets):

```{r}
#we want to be sure that the distribution is as we defined to have a correct classification
prop.table(table(seeds_train$Type))
prop.table(table(seeds_test$Type))
```

#### target factor (label) vector for classification
# Decision tree

The 8th column of the dataset is the type class variable, so we need to exclude it from the training data frame, but supply it as the target factor (label) vector for classification:

```{r}
# apply model in training data (8th column is the label to be predicted)
#creation of decision tree and we do not take the seeds (8)
seeds_model <- C5.0(seeds_train[-8], seeds_train$Type)

seeds_model
```
<<<<<<< HEAD
The tree size is 6 and it contains 7 predictors.
=======

##### simple facts about the tree
The preceding text shows some simple facts about the tree, including the function
call that generated it, the number of features (labeled predictors), and examples
(labeled samples) used to grow the tree. Also listed is the tree size of 7, which
indicates that the tree is 7 decisions deep 
>>>>>>> d969d9d6123bf03d2377ec0523df743bd3513325

Next we'll look at the summary of the model. 

```{r}
#summary of the model to understand it
summary(seeds_model)
```
### Explanation of the tree:

There are only 2 possibilities of the kernel groove is greater than 5.528,
The kernel groove <=5.528, there is one possibility of Area > 13.37, otherwise, Area<=13.37.
In this case, there is only one possibility to Kernel.Groove <= 4.783, otherwise, one possibility of Kernel.Groove > 4.783 , otherwise, Kernel.Groove<=4.783 and one possibility for Asymmetry.Coeff <= 1.502, otherwise, Asymmetry.Coeff> 1.502 and3 possibilities to kernel.Groove>4.914, otherwise, kernel.groove<=13.914 and 3 possibilities for perimeter<=13.47 and 1 possibility foe Perimeter>13.47.

the percentage of error is relatively low: 1.3% 
Kernel.Groove and area seem to be the better attribute. To construct the tree, the model used 4 attributes: Kernel.Groove, area, Asymmetry.Coeff and Kernel.Width.

We want to evaluate our prediction

```{r}
# apply model on test data
seeds_pred <- predict(seeds_model, seeds_test)

CrossTable(seeds_test$Type, seeds_pred, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c('actual type', 'predicted type'))
```
Type seed 2 and 3 are predicted correctly at 100%, there is no false positive or false negative. The type seed 1 has some false negatives,but few.

We will try to improve our results, using adaptive boosting to have a lower percentage of error. This is a process in which many decision trees are built and the trees vote on the best class for each example.

### Adaptive Boosting
 
We'll start with 10 trials, a number that has become the de facto standard, as research 
suggests that this reduces error rates on test data by about 25 percent:

```{r}
# boosting with 10 trials (on training)

seeds_boost10 <- C5.0(seeds_train[-8], seeds_train$Type, trials = 10)

seeds_boost10 

summary(seeds_boost10 )
```

The classifier made 1 mistake for an error rate of
0.6% percent. This is an improvement over the previous training error rate
before adding boosting! However, it remains to be seen whether we see
a similar improvement on the test data. Let's take a look:

```{r}
# boosting on test data
seeds_boost_pred10 <- predict(seeds_boost10, seeds_test)

CrossTable(seeds_test$Type, seeds_boost_pred10,
prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('actual Type', 'predicted Type'))
```

The model made 2 errors instead of 3 before the boost, on seeds of type 1.
WE will try to improve the result, with more trials than 10, for example 15.

```{r}
# boosting with 20 trials (on training)

seeds_boost20 <- C5.0(seeds_train[-8], seeds_train$Type, trials = 20)

seeds_boost20 

summary(seeds_boost20 )


# boosting on test data
seeds_boost_pred20 <- predict(seeds_boost20, seeds_test)

CrossTable(seeds_test$Type, seeds_boost_pred20,
prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('actual Type', 'predicted Type'))

```
With 10 trials, we obtained the best result. Indeed, choosing 20 trials in the boost does not improve the result. The error rate stay similar than with 10 trials.

# then we should write our conclusion about the different predictions obtained.
Compare these results to the boosted model; this version makes more mistakes overall, but the types of mistakes are very different. Where the previous models incorrectly classified a small number of defaults correctly, our weighted model has does much better in this regard. This trade resulting in a reduction of false negatives at the expense of increasing false positives may be acceptable if our cost estimates were accurate.
To create our decision trees in this practice we used the C5.0 package. 


2. KNN Algorithm

First we will work with a new table of data. 
Then,we will transform the values of the Type variable  to a factor variable:

```{r}

seeds$Type <- factor(seeds$Type, levels = c("1", "2","3"), labels = c("1", "2","3"))

# percentage of each type
round(prop.table(table(seeds$Type))*100, digits = 1)
```
### Normalization
We'll have to normalize our data to make sure our classifier will work. 
Lets create a simple normalization function (min/max normalization):

#Lilach, we cannot normalized for the decision tree, because before to normalize you need to remove the Types column and we need this column in the decision tree. So, I replaced the normalization in the KNN algo.

```{r}
# create the min/max normalization function:
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}

# test our function
normalize(c(1, 2, 3, 4, 5))
normalize(c(10, 20, 30, 40, 50))
```

Use the normalization function on our data, and make sure it worked:

```{r}
seeds_norm <- as.data.frame(lapply(seeds[1:7], normalize))

summary(seeds_norm$Area)
```
Since we want to use KNN, we have to make sure the range of values of each parameter is similar. 

The next step is to create a training set and a test set, and to store the labels in a different vector to be used later. We usually want to divide our data into 80%, 20% test, but we can always try a ratio of 85-15 or even 90-10 
and see if results improved. 

```{r}

# types of train set

seeds_train_types <- seeds[sample((nrow(seeds)), 159), 8]

# labels of test set
seeds_test_types <- seeds[sample((nrow(seeds)), 160:199), 8]
```

Now, we will run the KNN algorithm and We will start with K of 3, because we have 3 types of seeds

#Now, we will run the KNN algorithm and We will start with K of 12, the square root of 159 and also an odd number, #reducing the chance of a tie vote.

```{r}
seeds_test_pred <- knn(train = seeds_train, test = seeds_test, cl = seeds_train_types, k=3)
```

 Since the knn function returns a factor vector of the predicted values, we'll compare that vector with the true labels we saved in advance. We'll do the comparison with the CrossTable function, from the gmodels package we'll load: 

```{r}
# it is not running
CrossTable(x = seeds_test_types, y = seeds_test_pred, prop.chisq=FALSE)
```
#Lilach, here I do not understand why we have only one row, we should have 3 rows: one for each type.

In the above table we see that all of the benign were predicted correctly, and 36 out of the 39 malignant 
were too. 3 malignant labels were mistaken for being benign.

Lets see if we can improve our results. First, lets try z-score standardization instead of normalization:

```{r}
# z-score normalization
seeds_z <- as.data.frame(scale(seeds[-8]))

summary(seeds_z$Area)
```

We'll quickly repeat the previous steps (divide the data into train and test, classify, and compare the results to the true values):


```{r}
seeds_train_z <- seeds_z[1:159, ] #Lilach there is a problem here , we should sample data but when I sample them I have erors in the crosstable.
seeds_test_z <- seeds_z[160:199, ]#same
seeds_train_types_z <- seeds[1:159,8]
seeds_test_types_z <- seeds[160:199, 8]
seeds_test_pred_z <- knn(train = seeds_train_z, test = seeds_test_z, cl = seeds_train_types_z, k=3)
CrossTable(x = seeds_test_types_z, y = seeds_test_pred_z, prop.chisq=FALSE)
```

It seems to be worth, the model predicted only 2 types on 3. What about a different value of K?

```{r}
seeds_test_pred_k8 <- knn(train = seeds_knn_train, test = seeds_knn_test, cl = seeds_train_types, k=8)
seeds_test_pred_k15 <- knn(train = seeds_knn_train, test = seeds_knn_test, cl = seeds_train_types, k=15)
seeds_test_pred_k1 <- knn(train = seeds_knn_train, test = seeds_knn_test, cl = seeds_train_types, k=1)

CrossTable(x = seeds_train_types, y = seeds_test_pred_k8, prop.chisq=FALSE)
CrossTable(x = seeds_train_types, y = seeds_test_pred_k15, prop.chisq=FALSE)
CrossTable(x = seeds_train_types, y = seeds_test_pred_k1, prop.chisq=FALSE)

```
# error... need help