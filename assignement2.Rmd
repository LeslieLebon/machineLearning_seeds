---
title: "assignement2"
author: "Lilach Herzog & Leslie Cohen"
date: "13 5 2022"
output: 
  pdf_document: 
    toc_depth: 4
  word_document: 
    toc: yes
    toc_depth: 4
    fig_caption: yes
editor_options: 
  chunk_output_type: inline
---

Our objective was to analyze the “wheat-seeds” dataset (where the 'Type' of the seed is the column used for labeling and classifying) using two classifier models that we built. We decided on using the KNN algorithm and the decision tree (DT) algorithm.

# preproccessing

## Upload data and libraries

```{r, message=FALSE, warning=FALSE}
library(gmodels)
library(C50)
library(class) 

seeds <- read.csv("seeds.csv")# read data
set.seed(1234)#to initialize a pseudo random number generator. 
```

## initial look at the data

We know the class ‘type’ is the column (label) to classify according to. we took a quick look at the data:
```{r, message=FALSE, warning=FALSE}
str(seeds)
```
There are 3 types of seeds (in the column 'Type'): 1, 2 and 3.

Since the whole table is sorted according to the' type' column, and we want to work randomly, we will shuffle the whole table

```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
seeds <- seeds[sample(nrow(seeds)),]
str(seeds) #look at the data again to make sure it's shuffled
```
In order to further understand the data and be able to work with it we used the "table" function. this function builds a contingency table of the counts at each combination of factor levels- what interests us is the Type:
```{r, message=FALSE, warning=FALSE}
table(seeds$Type)
```
We can see the Type has 66 times type 1, 68 times type 2, and 65 times type 3, about a third of each type.

Before starting on teaching the algorithm, we converted the 'Type' column to a factor, as that is what is required by the C50 package.
```{r, message=FALSE, warning=FALSE}
seeds$Type<-as.factor(seeds$Type)
```
Now- let's get to work!!

## split into training and test sets
```{r, message=FALSE, warning=FALSE}
length_of_training_set <-round(0.8*(nrow(seeds)),0) #  0.8% of the observations out of the total 199
seeds_train <- seeds[1:length_of_training_set, ]
seeds_test <- seeds[(length_of_training_set+1):nrow(seeds), ]
seeds_train_labels <-seeds_train[, 8]
seeds_test_labels <-seeds_test[, 8]
```
Since we know that there are about a third of each type in the data set, we want to be sure that the distribution is as we defined (about 1/3 of each type in of both training and test sets):

```{r, echo=FALSE, message=FALSE, warning=FALSE}
prop.table(table(seeds_train$Type))
prop.table(table(seeds_test$Type))
```

### target factor (label) vector for classification
# Decision tree
A Decision Tree uses a series of questions that lead to a decision- a class. The goal is to improve purity, to try and have a higher and higher percentage of items in the node are similar, until all items in a node belong to the same class. It does so by taking the items in a node and splitting them in such a manner that they become two groups with improved purity in each (more samples in the group belong to the same class). 

We will use the C5.0 method.
The 8th column of the dataset is the type class variable, so we need to exclude it from the training data frame, and supply it as the target factor (label) vector for classification:
```{r, message=FALSE, warning=FALSE}
seeds_model <- C5.0(seeds_train[-8], seeds_train$Type)
```

### Explanation of initial tree:
```{r echo=FALSE, message=FALSE, warning=FALSE}
#summary of the model to understand it
summary(seeds_model)
```

The algorithm did six iterations. In the first iteration it divided all the seeds into two subgroups (nodes) according to whether their Groove was higher or lower than 5.533. One node was pure (all seeds in the node were Class 2), and the other node went through another iteration according to the Area, where both nodes were not pure and needed to go through another iteration. One node was split into two pure nodes-again according to Groove-which yielded 2 pure nodes (Class 3 or class 1). The other node was split according to the asymmetry coefficient which resulted in one pure node (of Class 1 ) whereas the other node went into another iteration according to the width which yielded 2 pure nodes (classes 3 and 1 ).

the percentage of error is relatively low: 1.3%.
The algorithm uses the feature that provides the most Information Gain in every split.Since it used 100.00% of the information from the Kernel.Groove column, we can assume that that column provided the most information gain (at least in the first iteration). The algorithm also used  66.04% of the Area column; 31.45% of the Asymmetry.Coeff; and 4.40%	of the Kernel.Width column, so that also gives us some information about how much these features influenced the Information Gain function.

After teaching the model on the training set, we want to use it to predict the class is on the test set, and then compare the test set to what we predicted for the test said using our predictive model.

```{r, message=FALSE, warning=FALSE}
# apply model on test data
seeds_pred <- predict(seeds_model, seeds_test)
CrossTable(seeds_test$Type, seeds_pred, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c('actual type', 'predicted type'))
```
Type seeds 2 and 3 both have no false positives (all predictions of type two or three were actually typed two or 3) with a few false negatives (one type 2 and 2 type 3s that were predicted as type one). Type 1 on the other hand has no false negative (all type 1 seeds were predicted as type 1) but it had a few false positives (as mentioned before- one that was actually type 2 and two were actually type 3).

We will try to improve our results, using adaptive boosting to have a lower percentage of error. This is a process in which many decision trees are built and the trees vote on the best class for each example.

## Adaptive Boosting
We'll start with 10 trials, a number that has become the de facto standard, as research suggests that this reduces error rates on test data by about 25 percent:

```{r, results=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
# boosting with 10 trials (on training)
seeds_boost10 <- C5.0(seeds_train[-8], seeds_train$Type, trials = 10)
summary(seeds_boost10 )
```

The classifier made 1 mistake for an error rate of
0.6% percent. This is an improvement over the previous training error rate
before adding boosting! However, it remains to be seen whether we see
a similar improvement on the test data. Let's take a look:

```{r, message=FALSE, warning=FALSE}
# boosting on test data
seeds_boost_pred10 <- predict(seeds_boost10, seeds_test)

CrossTable(seeds_test$Type, seeds_boost_pred10,
prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('actual Type', 'predicted Type'))
```

The model made 2 errors instead of 3 before the boost, on seeds of type 1.
WE will try to improve the result, with more trials than 10, for example 15.

```{r, message=FALSE, warning=FALSE, results=FALSE}
# boosting with 20 trials (on training)
seeds_boost20 <- C5.0(seeds_train[-8], seeds_train$Type, trials = 20)
summary(seeds_boost20 )

# boosting on test data
seeds_boost_pred20 <- predict(seeds_boost20, seeds_test)
CrossTable(seeds_test$Type, seeds_boost_pred20,
prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('actual Type', 'predicted Type'))

```
With 10 trials, we obtained the best result. Indeed, choosing 20 trials in the boost does not improve the result. The error rate stay similar than with 10 trials.

### DT conclusion

then we should write our conclusion about the different predictions obtained.
Compare these results to the boosted model; this version makes more mistakes overall, but the types of mistakes are very different. Where the previous models incorrectly classified a small number of defaults correctly, our weighted model has does much better in this regard. This trade resulting in a reduction of false negatives at the expense of increasing false positives may be acceptable if our cost estimates were accurate.
To create our decision trees in this practice we used the C5.0 package. 


# KNN Algorithm
In the KNN Algorithm, k is a user-defined constant, and an unlabeled vector (a query or test point) is classified by assigning the label which is most frequent among the k training samples nearest to that query point.
## Normalization
In order to compare the distances of different features with different scales, we made sure the range of values of each parameter is similar by using a simple normalization function (min/max normalization):
```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
# min/max normalization function:
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
```
and to test our function:
```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
normalize(c(1, 2, 3, 4, 5))
normalize(c(10, 20, 30, 40, 50))
```

We used the normalization function on our training and test sets, and made sure it worked:
```{r, message=FALSE, warning=FALSE}
normalized_seeds_train <- as.data.frame(lapply(seeds_train[1:7], normalize))
normalized_seeds_test <- as.data.frame(lapply(seeds_test[1:7], normalize))
summary(normalized_seeds_train$Area)
summary(normalized_seeds_test$Area)
```

## run the KNN algorithm
Now, we will run the KNN algorithm and We will start with K of 3, because we have 3 types of seeds

And finally, we are ready to run our algorithm. We'll start with K of 21, the square root of 455 and also an odd number, reducing the change of a tie vote.
```{r, message=FALSE, warning=FALSE}
seeds_test_pred <- knn(train = normalized_seeds_train, test = normalized_seeds_test, cl = seeds_train_labels, k=21)
```

 Since the knn function returns a factor vector of the predicted values, we'll compare that vector with the true labels we saved in advance. We'll do the comparison with the CrossTable function, from the gmodels package loaded: 

```{r, message=FALSE, warning=FALSE}
# it is not running
CrossTable(x = seeds_test_labels, y = seeds_test_pred, prop.chisq=FALSE)
```
In the above table we see that all 17 seeds(100%) of type 1 were predicted correctly, but it has a few false positives (3 seeds that were wrongly predicted as Type one: one seed was actually type 2 and 2 were type 3).
on the other hand- types two and three which had no false positives (all seeds that were predicted as two or three were actually two or 3 respectively) and only a few false negatives (14/15 type 2 seeds (93.3%) and 6/8 type 3 seeds (75%) were identified correctly). 
these are really good results!!
Lets see if we can improve our results. First, lets try z-score standardization instead of normalization:

```{r, message=FALSE, warning=FALSE, results=FALSE}
seeds_train_z <- as.data.frame(scale(seeds_train[1:7]))
seeds_test_z <- as.data.frame(scale(seeds_test[1:7]))
seeds_test_pred_z <- knn(train = seeds_train_z, test = seeds_test_z, cl = seeds_train_labels, k=3)
CrossTable(x = seeds_test_labels, y = seeds_test_pred_z, prop.chisq=FALSE)
```
Not much of an improvement, maybe even worse.
We tried several more times with different K's:
```{r, message=FALSE, warning=FALSE, results=FALSE}
seeds_test_pred_k3 <- knn(train = normalized_seeds_train, test = normalized_seeds_test, cl = seeds_train_labels, k=3)
CrossTable(x = seeds_test_labels, y = seeds_test_pred_k3, prop.chisq=FALSE)

seeds_test_pred_k9 <- knn(train = normalized_seeds_train, test = normalized_seeds_test, cl = seeds_train_labels, k=9)
CrossTable(x = seeds_test_labels, y = seeds_test_pred_k9, prop.chisq=FALSE)

seeds_test_pred_k15 <- knn(train = normalized_seeds_train, test = normalized_seeds_test, cl = seeds_train_labels, k=15)
CrossTable(x = seeds_test_labels, y = seeds_test_pred_k15, prop.chisq=FALSE)

seeds_test_pred_k21 <- knn(train = normalized_seeds_train, test = normalized_seeds_test, cl = seeds_train_labels, k=21)
CrossTable(x = seeds_test_labels, y = seeds_test_pred_k21, prop.chisq=FALSE)
```

 It looks like the results are exactly the same, except for K = 9 we got 7/8 type 3 seeds (87.5%) that were predicted correctly and only once predicted falsely as one (which improved both the true positive on type 3 and the false negative on type one).

### feature selection
The DT algorithm uses the feature that provides the most Information Gain in every split. Using that logic, the feature that is used the least provides the least information gain, and if it doesn't add much information it might disturb as noise. Thus, since we know that the accuracy of the k-NN algorithm can be severely degraded by the presence of noisy or irrelevant features, we tried the KNN algorithm once more, with K=9 and without the "Kernel.Length" column (which attributed the least to the building of the DT). 
```{r, message=FALSE, warning=FALSE}
# z-score normalization
feature_selected_train <- seeds_train[-4]
feature_selected_test <- seeds_test[-4]
feature_selected_test_pred <- knn(train = seeds_train_z, test = seeds_test_z, cl = seeds_train_labels, k=9)
CrossTable(x = seeds_test_labels, y = feature_selected_test_pred, prop.chisq=FALSE)
```
We managed to improve the true positive of type three to 100%! We have only one mistake left (a type two seed that was predicted as type 1).
## KNN conclusions
We managed to classify our data using the KNN algorithm. We saw that the best results were with K=9 and without the "Kernel.Length" column, and we managed to classify correctly all seeds except for one.

# total conclusions
