---
title: "assignement2"
author: "Lilach Herzog & Leslie Cohen"
date: "13 5 2022"
output: 
  pdf_document: 
    toc_depth: 4
  word_document: 
    toc: yes
    toc_depth: 4
    fig_caption: yes
editor_options: 
  chunk_output_type: inline
---

Our objective was to analyze the “wheat-seeds” dataset (where the 'Type' of the seed is the column used for labeling and classifying) using two classifier models that we built. We decided on using the KNN algorithm and the decision tree (DT) algorithm.

# preproccessing

## Upload data and libraries

```{r, message=FALSE, warning=FALSE}
library(gmodels)
library(C50)
library(class) 

seeds <- read.csv("seeds.csv")# read data
set.seed(1234)#to initialize a pseudo random number generator. 
```

## initial look at the data

We know the class ‘type’ is the column (label) to classify according to. we took a quick look at the data:
```{r, message=FALSE, warning=FALSE}
str(seeds)
```
There are 3 types of seeds (in the column 'Type'): 1, 2 and 3.

Since the whole table is sorted according to the' type' column, and we want to work randomly, we will shuffle the whole table

```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
seeds <- seeds[sample(nrow(seeds)),]
str(seeds) #look at the data again to make sure it's shuffled
```
In order to further understand the data and be able to work with it we used the "table" function. this function builds a contingency table of the counts at each combination of factor levels- what interests us is the Type:
```{r, message=FALSE, warning=FALSE}
table(seeds$Type)
```
We can see the Type has 66 times type 1, 68 times type 2, and 65 times type 3, about a third of each type.

Before starting on teaching the algorithm, we converted the 'Type' column to a factor, as that is what is required by the C50 package.
```{r, message=FALSE, warning=FALSE}
seeds$Type<-as.factor(seeds$Type)
```
Now- let's get to work!!

## split into training and test sets
```{r, message=FALSE, warning=FALSE}
length_of_training_set <-round(0.8*(nrow(seeds)),0) #  0.8% of the observations out of the total 199
seeds_train <- seeds[1:length_of_training_set, ]
seeds_test <- seeds[(length_of_training_set+1):nrow(seeds), ]
```
Since we know that there are about a third of each type in the data set, we want to be sure that the distribution is as we defined (about 1/3 of each type in of both training and test sets):

```{r, echo=FALSE, message=FALSE, warning=FALSE}
prop.table(table(seeds_train$Type))
prop.table(table(seeds_test$Type))
```

### target factor (label) vector for classification
# Decision tree

The 8th column of the dataset is the type class variable, so we need to exclude it from the training data frame, but supply it as the target factor (label) vector for classification:

```{r, message=FALSE, warning=FALSE}
seeds_model <- C5.0(seeds_train[-8], seeds_train$Type)
```


### Explanation of initial tree:
```{r echo=FALSE, message=FALSE, warning=FALSE}
#summary of the model to understand it
summary(seeds_model)
```

The algorithm divided the dataset each iteration into two parts: it chooses a number and a column and everything that is greater than that number in that column goes into one group everything that is smaller or equal to that number in that column goes to the other group. It keeps them dividing until all the members in a group have the same type.

- The algorithm started with the column' groove', and the number 5.33. It found that all those with a groove bigger than that are of type 2 (sub-mission completed). 
The algorithm then went to classify the data in the other group (groove smaller than 5.533). 
- It divided that group into two subgroups according to the column' area' and found that all those left that are larger than an area of 13.37 are all 1. 
- The ones that are smaller or equal to in area of 13.37 and groove of 5.33 were again divided by the groove. Out of those with a groove that is bigger than 4.914, those with the asymmetry coefficient that is smaller or equal to 1.791 or a type one, and those that are larger than that art type 3.
- The seeds with a groove that is larger than 4.915 (and smaller than 5.533 ) are divided according to the perimeter. Those were the perimeter larger than 13.41 or type 1, those that are smaller or equal to 13 point .1 and larger than 13.19 are type 3 and those that are smaller than 13.19 are type one.

the percentage of error is relatively low: 1.3% 
Kernel.Groove and area seem to be the better attribute. To construct the tree, the model used 4 attributes: Kernel.Groove, area, Asymmetry.Coeff and Kernel.Width.

We want to evaluate our prediction

```{r, message=FALSE, warning=FALSE}
# apply model on test data
seeds_pred <- predict(seeds_model, seeds_test)

CrossTable(seeds_test$Type, seeds_pred, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c('actual type', 'predicted type'))
```
Type seed 2 and 3 are predicted correctly at 100%, there is no false positive or false negative. The type seed 1 has some false negatives,but few.

We will try to improve our results, using adaptive boosting to have a lower percentage of error. This is a process in which many decision trees are built and the trees vote on the best class for each example.

### Adaptive Boosting
 
We'll start with 10 trials, a number that has become the de facto standard, as research 
suggests that this reduces error rates on test data by about 25 percent:

```{r, message=FALSE, warning=FALSE}
# boosting with 10 trials (on training)

seeds_boost10 <- C5.0(seeds_train[-8], seeds_train$Type, trials = 10)

seeds_boost10 

summary(seeds_boost10 )
```

The classifier made 1 mistake for an error rate of
0.6% percent. This is an improvement over the previous training error rate
before adding boosting! However, it remains to be seen whether we see
a similar improvement on the test data. Let's take a look:

```{r, message=FALSE, warning=FALSE}
# boosting on test data
seeds_boost_pred10 <- predict(seeds_boost10, seeds_test)

CrossTable(seeds_test$Type, seeds_boost_pred10,
prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('actual Type', 'predicted Type'))
```

The model made 2 errors instead of 3 before the boost, on seeds of type 1.
WE will try to improve the result, with more trials than 10, for example 15.

```{r, message=FALSE, warning=FALSE}
# boosting with 20 trials (on training)

seeds_boost20 <- C5.0(seeds_train[-8], seeds_train$Type, trials = 20)

seeds_boost20 

summary(seeds_boost20 )


# boosting on test data
seeds_boost_pred20 <- predict(seeds_boost20, seeds_test)

CrossTable(seeds_test$Type, seeds_boost_pred20,
prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('actual Type', 'predicted Type'))

```
With 10 trials, we obtained the best result. Indeed, choosing 20 trials in the boost does not improve the result. The error rate stay similar than with 10 trials.

### DT conclusion

then we should write our conclusion about the different predictions obtained.
Compare these results to the boosted model; this version makes more mistakes overall, but the types of mistakes are very different. Where the previous models incorrectly classified a small number of defaults correctly, our weighted model has does much better in this regard. This trade resulting in a reduction of false negatives at the expense of increasing false positives may be acceptable if our cost estimates were accurate.
To create our decision trees in this practice we used the C5.0 package. 


# KNN Algorithm

## preproccessing
First we will work with a new table of data. 
Then,we will transform the values of the Type variable  to a factor variable:

```{r, message=FALSE, warning=FALSE}

seeds$Type <- factor(seeds$Type, levels = c("1", "2","3"), labels = c("1", "2","3"))

# percentage of each type
round(prop.table(table(seeds$Type))*100, digits = 1)
```
### Normalization
We'll have to normalize our data to make sure our classifier will work. 
Lets create a simple normalization function (min/max normalization):


```{r, message=FALSE, warning=FALSE}
# min/max normalization function:
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}

# test our function
normalize(c(1, 2, 3, 4, 5))
normalize(c(10, 20, 30, 40, 50))
```

Use the normalization function on our data, and make sure it worked:

```{r, message=FALSE, warning=FALSE}
seeds_norm <- as.data.frame(lapply(seeds[1:7], normalize))

summary(seeds_norm$Area)
```
Since we want to use KNN, we have to make sure the range of values of each parameter is similar. 

The next step is to create a training set and a test set, and to store the labels in a different vector to be used later. We usually want to divide our data into 80%, 20% test, but we can always try a ratio of 85-15 or even 90-10 
and see if results improved. 

```{r, message=FALSE, warning=FALSE}

# types of train set


samples_seed <- seeds[sample(nrow(seeds)),]
seeds_train_types <-samples_seed[1:159, 8]

# labels of test set

seeds_test_types <- samples_seed[160:199,8 ]
```

Now, we will run the KNN algorithm and We will start with K of 3, because we have 3 types of seeds

## run the KNN algorithm
Now, we will run the KNN algorithm and We will start with K of 12 (???), the square root of 159 and also an odd number, #reducing the chance of a tie vote.

```{r, message=FALSE, warning=FALSE}
seeds_test_pred <- knn(train = seeds_train, test = seeds_test, cl = seeds_train_types, k=3)
```

 Since the knn function returns a factor vector of the predicted values, we'll compare that vector with the true labels we saved in advance. We'll do the comparison with the CrossTable function, from the gmodels package we'll load: 

```{r, message=FALSE, warning=FALSE}
# it is not running
CrossTable(x = seeds_test_types, y = seeds_test_pred, prop.chisq=FALSE)
```


```{r, message=FALSE, warning=FALSE}
seeds_test_pred <- knn(train = seeds_train, test = seeds_test, cl = seeds_train_types, k=9)
```

 Since the knn function returns a factor vector of the predicted values, we'll compare that vector with the true labels we saved in advance. We'll do the comparison with the CrossTable function, from the gmodels package we'll load: 

```{r, message=FALSE, warning=FALSE}
# it is not running
CrossTable(x = seeds_test_types, y = seeds_test_pred, prop.chisq=FALSE)
```

```{r, message=FALSE, warning=FALSE}
seeds_test_pred <- knn(train = seeds_train, test = seeds_test, cl = seeds_train_types, k=18)
```

 Since the knn function returns a factor vector of the predicted values, we'll compare that vector with the true labels we saved in advance. We'll do the comparison with the CrossTable function, from the gmodels package we'll load: 

```{r, message=FALSE, warning=FALSE}
# it is not running
CrossTable(x = seeds_test_types, y = seeds_test_pred, prop.chisq=FALSE)
```


Lets see if we can improve our results. First, lets try z-score standardization instead of normalization:

```{r, message=FALSE, warning=FALSE}
# z-score normalization
seeds_z <- as.data.frame(scale(samples_seed[-8]))

summary(seeds_z$Area)
```

We'll quickly repeat the previous steps (divide the data into train and test, classify, and compare the results to the true values):


```{r, message=FALSE, warning=FALSE}
seeds_train_z <- seeds_z[1:159, ] 
seeds_test_z <- seeds_z[160:199, ]
seeds_train_types_z <- samples_seed[1:159,8]
seeds_test_types_z <- samples_seed[160:199, 8]
seeds_test_pred_z <- knn(train = seeds_train_z, test = seeds_test_z, cl = seeds_train_types_z, k=3)
CrossTable(x = seeds_test_types_z, y = seeds_test_pred_z, prop.chisq=FALSE)
```

It seems to be better, the model predicted  . 
## KNN conclusions
# total conclusions
